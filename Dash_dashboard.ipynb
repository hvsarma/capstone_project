{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic use libraries\n",
    "import re\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# for dashboard\n",
    "import panel as pn\n",
    "import param\n",
    "\n",
    "# for scraping the web\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from twitterscraper import query_tweets\n",
    "import twitterscraper\n",
    "\n",
    "# for visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# file management\n",
    "import csv\n",
    "import json\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "#for initial time-series modeling\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose as sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facebook Prophet libraries\n",
    "from fbprophet import Prophet\n",
    "from fbprophet.plot import plot_plotly, plot_cross_validation_metric\n",
    "from fbprophet.diagnostics import cross_validation, performance_metrics\n",
    "import plotly.offline as py\n",
    "pd.plotting.register_matplotlib_converters() #necessary to maintain pd.plotting functionality\n",
    "\n",
    "# for NLP\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from textblob import TextBlob "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dates_list(year=2018):\n",
    "    # define a list of dates for a given year\n",
    "    dates_year =[str(date)[:10] for date in pd.date_range(start=f'1/1/{year}', end=f'12/31/{year}')]\n",
    "    # define a list of dates for generating file names\n",
    "    dates_stripped_year = [date.replace('-','') for date in dates_year]\n",
    "    return dates_year, dates_stripped_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to clean tweets and get tweet sentiment\n",
    "\n",
    "# portions of the code below comes from :\n",
    "# https://towardsdatascience.com/extracting-twitter-data-pre-processing-and-sentiment-analysis-using-python-3-0-7192bd8b47cf\n",
    "def replace_emoticons(tweet):\n",
    "    \"This code replaces happy and sad emoticons with the words 'HAPPY' and 'SAD'\"\n",
    "    rhappy = '[' + re.escape(''.join(emoticons_happy)) + ']'\n",
    "    re.sub(rhappy, ' HAPPY ', tweet)\n",
    "    rsad = '[' + re.escape(''.join(emoticons_sad)) + ']'\n",
    "    re.sub(rsad, ' SAD ', tweet)\n",
    "    return tweet\n",
    "\n",
    "def clean_tweet(tweet): \n",
    "    ''' \n",
    "    Utility function to clean tweet text by removing links, usernames, and\n",
    "    special characters using simple regex statements. \n",
    "    '''\n",
    "    tweet = replace_emoticons(tweet)\n",
    "    # p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION)\n",
    "    # tweet = p.clean(tweet)\n",
    "    tweet = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t]) \\\n",
    "                            |(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "    return tweet\n",
    "\n",
    "def double_clean_tweet(tweet):\n",
    "    \"This function goes a little further than the previous clean function\"\n",
    "    #removing mentions\n",
    "    tweet = re.sub(r':', ' ', tweet)\n",
    "    tweet = re.sub(r'‚Ä¶', ' ', tweet)\n",
    "    #replace consecutive non-ASCII characters with a space\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
    "    #remove emojis from tweet  (unless you want to later go through the UNICODE\n",
    "    # charts and separate \"happy\" emojis from \"sad\" emojis and add them to \n",
    "    # the `replace_emoticons()` function)\n",
    "    tweet = emoji_pattern.sub(r'', tweet)\n",
    "    return tweet\n",
    "\n",
    "\n",
    "# Sentiment analysis code below adapted from:\n",
    "# https://www.geeksforgeeks.org/twitter-sentiment-analysis-using-python/\n",
    "def get_tweet_sentiment(tweet): \n",
    "    ''' \n",
    "    Utility function to classify sentiment of passed tweet \n",
    "    using textblob's sentiment method \n",
    "    '''\n",
    "    # create TextBlob object of passed tweet text \n",
    "    analysis = TextBlob(tweet)\n",
    "    # set sentiment \n",
    "    polarity = analysis.sentiment.polarity\n",
    "    subjectivity = analysis.sentiment.subjectivity\n",
    "    if analysis.sentiment.polarity > 0.1: \n",
    "        sentiment = 'positive'\n",
    "    elif analysis.sentiment.polarity < -0.1: \n",
    "        sentiment = 'negative'\n",
    "    else: \n",
    "        sentiment = 'neutral'\n",
    "    return sentiment, polarity, subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to create a .CSV file that compiles the relevant \n",
    "# info from the JSONs, preprocesses the tweets, and performs sentiment analysis\n",
    "def json_to_csv_tweets(output_filename='output.csv', year=2018):\n",
    "    \"\"\"\n",
    "    Takes in JSON files of scraped tweets from the `./data/` folder,\n",
    "    cleans the tweets, performs sentiment analysis, and then outputs\n",
    "    the results to the provided destination CSV filename.\n",
    "    \"\"\"\n",
    "    # create the csv writer object\n",
    "    csvwriter = csv.writer(open(output_filename, 'w', newline=''))\n",
    "    csvwriter.writerow([\"timestamp\", \"text\", \"sentiment\", \"polarity\", \"subjectivity\", \"tally\"])\n",
    "\n",
    "    # iterate adding rows of JSON to the CSV file\n",
    "    dates_year, dates_stripped_year = make_dates_list(year)\n",
    "    for i in dates_stripped_year:\n",
    "        f = open(f'./data/t{i}.json')\n",
    "        data = json.load(f)\n",
    "        for tweet in data:\n",
    "            tw = tweet[\"text\"]\n",
    "            tw = replace_emoticons(tw)\n",
    "            tw = clean_tweet(tw)\n",
    "            tw = double_clean_tweet(tw)\n",
    "            sentiment, polarity, subjectivity = get_tweet_sentiment(tw)\n",
    "            csvwriter.writerow([i, tw, sentiment, polarity, subjectivity, 1])\n",
    "        f.close()\n",
    "        if float(i)%20 == 0:\n",
    "            print(f\"Finished working with:   ./data/t{i}.json\")\n",
    "    print(\"JOB IS COMPLETELY FINISHED.  HOORAY!!\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to scrape and clean financial data for a given year\n",
    "def fetch_data(symbol, object_type, stocks_apikey):\n",
    "    # Making an API request for a certain stock's history\n",
    "    if object_type == 'currency':\n",
    "        credentials = {'function':'FX_DAILY', \n",
    "                       'from_symbol':symbol, \n",
    "                       'to_symbol':'USD', \n",
    "                       'outputsize':'full',\n",
    "                       'apikey':stocks_API_key}\n",
    "    elif object_type == 'cryptocurrency':\n",
    "        credentials = {'function':'DIGITAL_CURRENCY_DAILY', \n",
    "                       'symbol':symbol, \n",
    "                       'market':'USD', \n",
    "                       'apikey':stocks_API_key}\n",
    "    else:\n",
    "        credentials = {'function':'TIME_SERIES_DAILY',\n",
    "                       'symbol':symbol,\n",
    "                       'outputsize':'full',\n",
    "                       'apikey':stocks_apikey}\n",
    "\n",
    "    r = requests.get('https://www.alphavantage.co/query', params=credentials)\n",
    "    # checking to make sure request was successful\n",
    "    print(r.status_code)\n",
    "    if r.status_code == requests.codes.ok:\n",
    "        print(\"Request Successful\")\n",
    "    return r\n",
    "\n",
    "\n",
    "def clean_financials(r, object_type):\n",
    "    # cleaning up the data to make it easier to work with\n",
    "    if object_type == 'currency':\n",
    "        df = pd.DataFrame(r.json()[\"Time Series FX (Daily)\"])\n",
    "    elif object_type == 'cryptocurrency':\n",
    "        df = pd.DataFrame(r.json()[\"Time Series (Digital Currency Daily)\"])\n",
    "    elif object_type in ['stock','index']:\n",
    "        df = pd.DataFrame(r.json()[\"Time Series (Daily)\"])\n",
    "\n",
    "    df = df.T.reset_index()\n",
    "\n",
    "    if object_type == 'cryptocurrency':\n",
    "        df.drop(columns=[\"1b. open (USD)\",\"2b. high (USD)\",\"3b. low (USD)\",\n",
    "            \"4b. close (USD)\",\"6. market cap (USD)\"], inplace=True, axis=1)\n",
    "\n",
    "    if object_type == 'currency':\n",
    "        df.columns = ['date','open','high','low','close']\n",
    "        df.date = pd.to_datetime(df.date)\n",
    "        df[['open','high','low','close']] = df[['open',\n",
    "            'high','low','close']].astype(float)\n",
    "    else:\n",
    "        df.columns = ['date','open','high','low','close','volume']\n",
    "        df.date = pd.to_datetime(df.date)\n",
    "        df[['open','high','low','close','volume']] = df[['open',\n",
    "            'high','low','close','volume']].astype(float)\n",
    "\n",
    "    # create a new column to account for after-hours trading\n",
    "    # this uses the next day's open value as the prior day's close value\n",
    "    cl24 = [df.loc[0].close]\n",
    "    for val in df.open.values:\n",
    "        cl24.append(val)\n",
    "    cl24 = pd.DataFrame(cl24[:-1], columns=['close_24'])\n",
    "    df = df.join(cl24)\n",
    "\n",
    "    # now we must account for when afterhours trading exceeds high/low values\n",
    "    df['high_24'] = df[['high', 'close_24']].values.max(1)\n",
    "    df['low_24'] = df[['low', 'close_24']].values.min(1)\n",
    "    # and add a few more columns that should be useful\n",
    "    df['range'] = df['high'] - df['low']\n",
    "    df['range_24'] = df['high_24'] - df['low_24']\n",
    "    df['change_24'] = df['close_24'] - df['open']\n",
    "    # setting date column as index to facilitate timeseries manipulation\n",
    "    df.set_index('date', inplace=True)\n",
    "    return df\n",
    "    \n",
    "\n",
    "def get_financial_data(symbol, object_type, stocks_apikey, year=2018, verbose=True):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    symbol         (string) Stock or Currency symbol\n",
    "    object_type    (string) must be one of these:\n",
    "                      'stock'\n",
    "                      'index'\n",
    "                      'currency'\n",
    "                      'cryptocurrency'\n",
    "    stocks_apikey  (string) your API key for \n",
    "                      https://www.alphavantage.co\n",
    "    year           (int) the year you wish to examine\n",
    "    verbose        if True, displays .info() and .head() of data\n",
    "    =========================================\n",
    "    Returns a DataFrame of daily financial information containing\n",
    "    at least opening, closing, high, and low values.\n",
    "    \"\"\"\n",
    "    valid_types = ['stock','index','currency','cryptocurrency']\n",
    "    if object_type in valid_types:\n",
    "        r = fetch_data(symbol, object_type, stocks_apikey)\n",
    "        df = clean_financials(r, object_type)    #cleaning data\n",
    "        year_df = df[f'{year}':f'{year}']        # getting 1 year's data\n",
    "        if verbose:\n",
    "            display(year_df.head(),year_df.info())\n",
    "        plt.figure(figsize=(15,5))\n",
    "        plt.plot(year_df['close_24'])\n",
    "        plt.title(f\"{symbol} Daily Performance for {year}\", fontsize=16)\n",
    "        plt.ylabel(\"Price (in USD)\");\n",
    "        return year_df\n",
    "    else:\n",
    "        print(\"\"\"\n",
    "        Invalid entry for 'object_type', must be one of these strings:\n",
    "                'stock'\n",
    "                'index'\n",
    "                'currency'\n",
    "                'cryptocurrency'\n",
    "        \"\"\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to decompose a time series, in order to detect\n",
    "# trends and seasonality, and allow for examining the residuals\n",
    "def df_decompose(df):\n",
    "    # Gather the trend, seasonality and noise of decomposed object\n",
    "    trend = sd(df).trend\n",
    "    seasonal = sd(df).seasonal\n",
    "    residual = sd(df).resid\n",
    "\n",
    "    # Plot gathered statistics\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.title(f\"Decomposition for {df}\")\n",
    "    plt.subplot(411)\n",
    "    plt.plot(df, label='Original', color=\"blue\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.subplot(412)\n",
    "    plt.plot(trend, label='Trend', color=\"blue\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.subplot(413)\n",
    "    plt.plot(seasonal, label='Seasonality', color=\"blue\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.subplot(414)\n",
    "    plt.plot(residual, label='Residuals', color=\"blue\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout();\n",
    "    \n",
    "    return residual.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot sentiment trends\n",
    "def plot_sentiments(csv_filename):\n",
    "    year=2018\n",
    "    tweets_df = pd.read_csv(csv_filename)\n",
    "    tweets_df.timestamp = pd.to_datetime(tweets_df.timestamp, format='%Y%m%d')\n",
    "    grouped = pd.DataFrame(tweets_df.groupby(['timestamp', 'sentiment'])['tally'].sum()).reset_index()\n",
    "    for sentiment in grouped.sentiment.unique():\n",
    "        temp_df = grouped[grouped.sentiment == sentiment].set_index('timestamp')\n",
    "        temp_df['tally'].plot(figsize=(15,8), label=sentiment)\n",
    "        plt.ylabel(\"Number of Tweets by Sentiment\\n(~1,000/day total)\", fontsize=16)\n",
    "        plt.title(f\"Daily Sentiment at midnight (UTC) in {year}\", fontsize=20)\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to automate the process of inspecting lunar trends\n",
    "\n",
    "def get_lunar_phases(year='2018'):\n",
    "    \"Given a year, returns a dataframe of lunar phases, dates, and times (UTC).\"\n",
    "    url = f\"https://aa.usno.navy.mil/cgi-bin/aa_phases.pl?year={year}&nump=65&format=t\"\n",
    "    res_page = requests.get(url)\n",
    "    soup = BeautifulSoup(res_page.content, 'html.parser')\n",
    "    table_cells = soup.find_all(\"td\")\n",
    "    output = pd.DataFrame(columns=['phase','date','time'])\n",
    "    for i in range(len(table_cells)):\n",
    "        row = np.floor(i/2)\n",
    "        if i%2 == 0:\n",
    "            output.at[row,'phase'] = table_cells[i].text\n",
    "        else:\n",
    "            output.at[row,'date'] = table_cells[i].text[:12]  #need to grab just beggining of string\n",
    "            output.at[row,'time'] = table_cells[i].text[-5:]  #need to grab just ending of string\n",
    "    output.date = pd.to_datetime(output.date)\n",
    "    output.reset_index(drop=True, inplace=True)\n",
    "    return output\n",
    "\n",
    "\n",
    "def lunar_phase_separator(phases_df, lower_window=0, upper_window=1):\n",
    "    \"\"\"\n",
    "    Converts DataFrame of moon phases into FBProphet-friendly format.\n",
    "    ---------------------\n",
    "    Inputs:\n",
    "    phases_df       DataFrame containing lunar phase dates for a given year\n",
    "                        (the output of the `get_lunar_phases()` function)\n",
    "    lower_window    (<=0) number of days prior to moon phase to include in 'holiday'\n",
    "    upper_window    (>=0) number of days after to moon phase to include in 'holiday'\n",
    "    ---------------------\n",
    "    Returns:        FBProphet-friendly DataFrame for use in 'holiday' parameter\n",
    "    \"\"\"\n",
    "    # let's separate the different moon phases\n",
    "    phase_names = ['Full Moon','Last Quarter','New Moon','First Quarter']\n",
    "    ph_list = []\n",
    "    for phase in phase_names:\n",
    "        moons = pd.DataFrame(phases_df.loc[phases_df['phase'] == phase]['date']).reset_index(drop=True)\n",
    "        moons.columns = ['ds']\n",
    "        moons['holiday'] = str(phase).lower().replace(\" \", \"\")\n",
    "        moons['lower_window'] = lower_window\n",
    "        moons['upper_window'] = upper_window\n",
    "        ph_list.append(moons)\n",
    "    phases = pd.concat((ph_list[0], ph_list[1], ph_list[2], ph_list[3]))\n",
    "    return phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to automate the process of inspecting lunar trends\n",
    "def prep_data_for_FBP(data, column_name):\n",
    "    \"\"\"\n",
    "    Given a DataFrame and the name of the column to be processed, \n",
    "    generates a FBProphet-ready DataFrame.\n",
    "    \"\"\"\n",
    "    d = data.reset_index()\n",
    "    prepped_data = d[['date', column_name]].sort_values(by=['date']).reset_index(drop=True)\n",
    "    prepped_data.columns = ['ds','y']\n",
    "    return prepped_data\n",
    "\n",
    "\n",
    "def cross_val_FBP(model, metric='rmse', show_metric_scores=False):\n",
    "    # cross validating using time horizons within the dataset\n",
    "    df_cv = cross_validation(model, initial='90 days', period='15 days', horizon = '30 days')\n",
    "    # performance metrics for the FBProphet model\n",
    "    df_p = performance_metrics(df_cv)\n",
    "    if show_metric_scores == True:\n",
    "        display(df_p.head())\n",
    "    # plotting performance metrics\n",
    "    fig = plot_cross_validation_metric(df_cv, metric)\n",
    "    pass\n",
    "\n",
    "\n",
    "def get_weekends(year='2018'):\n",
    "    weekends_df = pd.DataFrame(columns=['date','day_of_week'])\n",
    "    weekends_df.date = [date for date in pd.date_range(start=f'1/1/{year}', periods=470)]\n",
    "    weekends_df.day_of_week = [datetime.datetime(int(str(date)[:4]), \n",
    "        int(str(date)[5:7]), int(str(date)[8:10])).weekday() for date in weekends_df.date]\n",
    "    weekends_df = weekends_df[weekends_df.day_of_week >= 5].reset_index(drop=True)\n",
    "    return weekends_df\n",
    "\n",
    "\n",
    "def lunar_stock_trend(df, column_name, phases_df, year='2018', lower_window=0, \n",
    "                      upper_window=1, trades_on_weekends=False,\n",
    "                      cross_val=True, metric='rmse',\n",
    "                      show_metric_scores=False):\n",
    "    \"\"\"\n",
    "    This function takes in a DataFrame of stock data and the \n",
    "    column name for the feature to be examined, a DataFrame of lunar \n",
    "    phases for a year, the desired year, and the lower & upper \n",
    "    windows for 'holiday' dates.\n",
    "    \n",
    "    If the financial data contains values for weekends, set the\n",
    "    'trades_on_weekends' parameter to 'True'.\n",
    "    \n",
    "    Additional option to cross-validate FBProphet model predictions, and\n",
    "    select from a variety of metrics to use.\n",
    "    \n",
    "    ----------------------------------------------------\n",
    "    \n",
    "    Returns a list containing the model object, the 'future' dataframe\n",
    "    used to make predictions, the forecast output DataFrame, a graph\n",
    "    of the forecast, and a graph of forecast components.\n",
    "    \"\"\"\n",
    "    phases = lunar_phase_separator(phases_df, lower_window, upper_window)\n",
    "    if trades_on_weekends == False:\n",
    "        weekends_df = get_weekends(year)\n",
    "        phases = phases[~phases['ds'].isin(weekends_df.date)]\n",
    "    data = prep_data_for_FBP(df, column_name)\n",
    "    m = Prophet(holidays=phases)\n",
    "    m.fit(data)\n",
    "    future = m.make_future_dataframe(periods=60, freq='D')\n",
    "    if trades_on_weekends == False:\n",
    "        future = future[~future['ds'].isin(weekends_df.date)]\n",
    "    forecast = m.predict(future)\n",
    "    disp_length = 4 * (1 + abs(lower_window) + abs(upper_window))\n",
    "    display(forecast[(forecast['fullmoon'] + forecast['lastquarter'] + \n",
    "                      forecast['newmoon'] + forecast['firstquarter']).abs() > \n",
    "                     0][['ds', 'fullmoon', 'lastquarter', 'newmoon',\n",
    "                         'firstquarter']][:disp_length])        \n",
    "    fig1 = m.plot(forecast);\n",
    "    fig1.set_size_inches(15, 5);\n",
    "    fig2 = m.plot_components(forecast)\n",
    "    fig2.set_size_inches(15, 10);\n",
    "    if cross_val:\n",
    "        cross_val_FBP(m, metric, show_metric_scores)\n",
    "        \n",
    "    return [m, future, forecast, fig1, fig2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_fbprophet(filename='tweets_nowords_2018.csv'):\n",
    "    print(filename)\n",
    "    tweets_df = pd.read_csv(filename)\n",
    "    tweets_df.timestamp = pd.to_datetime(tw_df.timestamp, format='%Y%m%d')\n",
    "    grouped = pd.DataFrame(tweets_df.groupby(['timestamp', 'sentiment'])['tally'].sum()).reset_index()\n",
    "\n",
    "    #prepare grouped sentiment data for FBProphet processing\n",
    "    #here, positive sentiment only\n",
    "    grp_pos = grouped[grouped.sentiment == 'positive'].drop('sentiment', axis=1).reset_index(drop=True)\n",
    "    grp_pos.columns = ['ds','y']\n",
    "\n",
    "    m = Prophet(holidays=phases)\n",
    "    m.fit(grp_pos)\n",
    "    future = m.make_future_dataframe(periods=60, freq='D')\n",
    "    forecast = m.predict(future)\n",
    "    forecast[(forecast['fullmoon'] + forecast['lastquarter'] + \n",
    "              forecast['newmoon'] + forecast['firstquarter']).abs() > \n",
    "             0][['ds', 'fullmoon', 'lastquarter', 'newmoon','firstquarter']][:10]\n",
    "    fig1 = m.plot(forecast);\n",
    "    fig1.set_size_inches(15, 5);\n",
    "    fig2 = m.plot_components(forecast)\n",
    "    fig2.set_size_inches(15, 10);\n",
    "    \n",
    "    # cross validating using time horizons within the dataset\n",
    "    df_cv = cross_validation(m, initial='90 days', period='15 days', horizon = '30 days')\n",
    "#     # performance metrics for the FBProphet model\n",
    "#     df_p = performance_metrics(df_cv)\n",
    "#     display(df_p.head())\n",
    "\n",
    "    # plotting performance metrics\n",
    "    fig3 = plot_cross_validation_metric(df_cv, metric='rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get most frequent words\n",
    "def get_top_n_words(corpus):\n",
    "    stopwords = set(ENGLISH_STOP_WORDS)\n",
    "    stopwords.update(['twitter','com','pic','ve','ll','just','like','don','really','00'])\n",
    "    vec = CountVectorizer(stop_words=stopwords).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq\n",
    "\n",
    "\n",
    "# function for plotting the most frequent words\n",
    "def plot_phase_words(corpus, keywords, phase, n=None):\n",
    "    comm_words = get_top_n_words(corpus.text)[:n]\n",
    "    df2 = pd.DataFrame(comm_words, columns=['text', 'count'])\n",
    "    total = df2['count'].sum()   ###################    \n",
    "    # plotting\n",
    "    fig, ax = plt.subplots(figsize=(15,5))\n",
    "    plt.xticks(rotation=30, fontsize=14)\n",
    "    hghts = ((df2['count'] / total) * 100)\n",
    "    rects = ax.bar(df2.text, hghts)                                                  #################\n",
    "    ax.set_title(f\"Top {n} Words for {phase} at Midnight UTC\", fontsize=16)\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.text(rect.get_x() + rect.get_width(), 1.005*height,\n",
    "                f'{np.round(height, 2)}%', ha='right', va='bottom')\n",
    "    plt.plot([], [], ' ', label=f\"Search Keywords:\\n{keywords}\")\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.show();\n",
    "\n",
    "\n",
    "# function to parse a CSV file and then plot all results\n",
    "def plot_CSV_words(filename, keywords, phases_df, n=None):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    filename       (str) CSV filename generated from Twitter scraping function\n",
    "    keywords       (str) the query keywords used in Twitter scraping function\n",
    "    phases_df      (df) dataframe of lunar phases for a given year\n",
    "    n              (int) number of most common words to return\n",
    "    ==================================================\n",
    "    Returns:\n",
    "    Histograms (5 total) of the 'n' most common words for each lunar phase and for\n",
    "    all the nights not in a lunar phase.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filename)\n",
    "    df.dropna(inplace=True)\n",
    "    df.timestamp = pd.to_datetime(df.timestamp, format='%Y%m%d')  \n",
    "    df.set_index('timestamp', inplace=True)\n",
    "    \n",
    "    phase_list = ['Full Moon','Last Quarter','New Moon','First Quarter','No Phase']\n",
    "    for phase in phase_list:\n",
    "        all_moons = list(phases_df.date.astype(str))\n",
    "        if phase != 'No Phase':\n",
    "            moon = list(phases_df[phases_df.phase == phase].date.astype(str))\n",
    "            moon_data = df.loc[df.index.floor('D').isin(moon)]\n",
    "            plot_phase_words(moon_data, keywords, phase, n=n)\n",
    "        else:\n",
    "            moon_data = df.loc[~df.index.floor('D').isin(all_moons)]\n",
    "            plot_phase_words(moon_data, keywords, phase, n=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code that has to be executed\n",
    "\n",
    "reading files and making dataframes and such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phases_2018_df = get_lunar_phases(year='2018')\n",
    "phases = lunar_phase_separator(phases_2018_df, lower_window=0, upper_window=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_df = pd.read_csv('tweets_happysad_2018.csv')\n",
    "tw_df.timestamp = pd.to_datetime(tw_df.timestamp, format='%Y%m%d')\n",
    "grouped = pd.DataFrame(tw_df.groupby(['timestamp', 'sentiment'])['tally'].sum()).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sentiment in grouped.sentiment.unique():\n",
    "#     print(sentiment)\n",
    "#     df_decompose(t2018_log[sentiment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sentiments('tweets_happysad_2018.csv', 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_API_key = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSFT_2018 = get_financial_data('MSFT', 'stock', stocks_API_key, year=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lunar_stock_trend(df=MSFT_2018,\n",
    "                  column_name='change_24',\n",
    "                  phases_df=phases_2018_df,\n",
    "                  year='2018',\n",
    "                  lower_window=-1,\n",
    "                  upper_window=1,\n",
    "                  trades_on_weekends=False,\n",
    "                  cross_val=True,\n",
    "                  metric='rmse', \n",
    "                  show_metric_scores=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_csv_files = ['tweets_lovehate_2018.csv',\n",
    "                   'tweets_happysad_2018.csv',\n",
    "                   'tweets_music_2018.csv',\n",
    "                   'tweets_money_2018.csv',\n",
    "                   'tweets_nowords_2018.csv',\n",
    "                   'tweets_politics_2018.csv',\n",
    "                   'tweets_coding_2018.csv']\n",
    "\n",
    "queries = ['love OR peace OR hate OR war',\n",
    "           'happy OR sad OR life OR death',\n",
    "           'music OR tunes OR dance',\n",
    "           'stocks OR money OR taxes',\n",
    "           '(no keywords entered)',\n",
    "           'politics OR government OR Trump',\n",
    "           'data science OR coding OR programming'] \n",
    "\n",
    "for filename, key_words in zip(tweet_csv_files, queries):\n",
    "    plot_CSV_words(filename=filename,\n",
    "                   keywords=key_words,\n",
    "                   phases_df=phases_2018_df,\n",
    "                   n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring daily positivity rates by query phrase....aggregated\n",
    "for search in tweet_csv_files:\n",
    "    plot_sentiments(search, 2018, positive_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing graphs for each query separately\n",
    "for search in tweet_csv_files:\n",
    "    plot_sentiments(csv_filename=search, year=2018, positive_only=False)\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running each CSV file through FBPophet\n",
    "for search in tweet_csv_files:\n",
    "    tweet_fbprophet(search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dash section\n",
    "\n",
    "some of the websites with info:\n",
    "   * Intro to Dash (blog) https://medium.com/plotly/introducing-dash-5ecf7191b503\n",
    "   * Interactive Dashboards w Dash (blog) https://alysivji.github.io/reactive-dashboards-with-dash.html\n",
    "   * Dash GitHub https://github.com/plotly/dash\n",
    "   * Dash documentation https://dash.plot.ly/?_ga=2.22784251.1143889031.1570652152-637402008.1568664543"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing .CSV data for use in Dash\n",
    "quers = ['lovehate','happysad','music','money','nowords','politics', 'coding']\n",
    "symbols = ['MSFT','EUR','BTC','SP500','SBAC']\n",
    "\n",
    "MSFT_df = pd.read_csv('MSFT.csv')\n",
    "MSFT_df.date = pd.to_datetime(MSFT_df.date, infer_datetime_format=True)\n",
    "EUR_df = pd.read_csv('EUR.csv')\n",
    "EUR_df.date = pd.to_datetime(EUR_df.date, infer_datetime_format=True)\n",
    "BTC_df = pd.read_csv('BTC.csv')\n",
    "BTC_df.date = pd.to_datetime(BTC_df.date, infer_datetime_format=True)\n",
    "SP500_df = pd.read_csv('SP500.csv')\n",
    "SP500_df.date = pd.to_datetime(SP500_df.date, infer_datetime_format=True)\n",
    "SBAC_df = pd.read_csv('SBAC.csv')\n",
    "SBAC_df.date = pd.to_datetime(SBAC_df.date, infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install dash==1.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install dash-daq==0.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "### This cell must be running in order to test whether ###\n",
    "###  the code below which has been saved as `app.py`   ###\n",
    "###   is properly functioning when output as HTML.     ###\n",
    "##########################################################\n",
    "!python app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output\n",
    "import pandas as pd\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "###Imports for FBProphet plot.ly functionality\n",
    "import numpy as np\n",
    "from fbprophet.diagnostics import performance_metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import (\n",
    "    MonthLocator,\n",
    "    num2date,\n",
    "    AutoDateLocator,\n",
    "    AutoDateFormatter)\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from pandas.plotting import deregister_matplotlib_converters\n",
    "deregister_matplotlib_converters()\n",
    "from plotly import tools as plotly_tools\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###############################\n",
    "##### Necessary functions #####\n",
    "###############################\n",
    "\n",
    "# Code adapted from: https://github.com/facebook/prophet/blob/master/python/fbprophet/plot.py\n",
    "def plot_holidays_component_plotly(m, fcst, figsize=(900, 300)):\n",
    "    \"\"\"Plot 'holidays' component of the forecast using Plotly.\n",
    "    ----- Parameters -----\n",
    "    m: Prophet model.\n",
    "    fcst: pd.DataFrame output of m.predict.\n",
    "    figsize: The plot's size (in px).\n",
    "    ------- Returns a Plotly Figure. -------\n",
    "    \"\"\"\n",
    "    range_margin = (fcst['ds'].max() - fcst['ds'].min()) * 0.05\n",
    "    range_x = [fcst['ds'].min() - range_margin, fcst['ds'].max() + range_margin]\n",
    "    text = None\n",
    "    fcst = fcst[fcst['holidays'] != 0].copy()\n",
    "    # Combine holidays into one hover text\n",
    "    holiday_features, _, _ = m.make_holiday_features(fcst['ds'], m.holidays)\n",
    "    holiday_features.columns = holiday_features.columns.str.replace('_delim_', '', regex=False)\n",
    "    holiday_features.columns = holiday_features.columns.str.replace('+0', '', regex=False)\n",
    "    text = pd.Series(data='', index=holiday_features.index)\n",
    "    for holiday_feature, idxs in holiday_features.iteritems():\n",
    "        text[idxs.astype(bool) & (text != '')] += '<br>'  # Add newline if additional holiday\n",
    "        text[idxs.astype(bool)] += holiday_feature\n",
    "    traces = []\n",
    "    traces.append(go.Scatter(\n",
    "        name='holidays',\n",
    "        x=fcst['ds'],\n",
    "        y=fcst['holidays'],\n",
    "        mode='lines',\n",
    "        line=go.scatter.Line(color='#0072B2', width=2),\n",
    "        text=text,\n",
    "    ))\n",
    "    xaxis = go.layout.XAxis(\n",
    "        title='Date',\n",
    "        type='date',\n",
    "        rangeslider={'visible': True},   ###this line may be a problem\n",
    "        range=range_x)\n",
    "    yaxis = go.layout.YAxis(rangemode='tozero',\n",
    "                            title='Lunar Phase Correlation',\n",
    "                            zerolinecolor='#AAA')\n",
    "    layout = go.Layout(\n",
    "        width=figsize[0],\n",
    "        height=figsize[1],\n",
    "        showlegend=False,\n",
    "        xaxis=xaxis,\n",
    "        yaxis=yaxis\n",
    "    )\n",
    "    figure = go.Figure(data=traces, layout=layout)\n",
    "    return figure\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#############################################\n",
    "#### BELOW IS THE CODE FOR THE DASHBOARD ####\n",
    "#############################################\n",
    "\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "markdown_paragraph = '''\n",
    "### About this Project\n",
    "\n",
    "#### By Matthew E. Parker\n",
    "\n",
    "Data Science Bootcamp Capstone Project for Flatiron School. \n",
    "For more information about this project, please read my \n",
    "[Medium article]('https://medium.com/@matthewparker_1059/modeling-lunar-cycles-in-tweets-and-financial-markets-using-facebook-prophet-d6ec0e9e20f'). \n",
    "If you wish to look at the code for yourself, please refer to the project's\n",
    "[GitHub repository]('https://github.com/magnawhale/capstone_project').\n",
    "\n",
    "For a fascinating (but slow-loading) sample exploration of Tweets, \n",
    "download and then open view my \n",
    "[Scattertext html file]('https://github.com/magnawhale/capstone_project/blob/master/Scattertext_nowords_example.html').\n",
    "Just continue loading if you get any error messages about responsiveness when opening this file.\n",
    "'''\n",
    "\n",
    "\n",
    "stopwords = set(ENGLISH_STOP_WORDS)\n",
    "stopwords.update(['twitter','com','pic','ve','ll','just','like','don','really','00'])\n",
    "\n",
    "tw_df = pd.read_csv('tw_sent.csv')\n",
    "tw_df.date = pd.to_datetime(tw_df.date, infer_datetime_format=True)\n",
    "\n",
    "moon_df = pd.read_csv('phases.csv')\n",
    "moon_df.date = pd.to_datetime(moon_df.date, infer_datetime_format=True)\n",
    "\n",
    "tw_word_freqs_df = pd.read_csv('tw_word_freqs.csv')\n",
    "tw_word_freqs_df['count'] = tw_word_freqs_df['count'].astype(int)\n",
    "\n",
    "queries = ['(no keywords entered)',\n",
    "           'love OR peace OR hate OR war',\n",
    "           'happy OR sad OR life OR death',\n",
    "           'music OR tunes OR dance',\n",
    "           'stocks OR money OR taxes',\n",
    "           'politics OR government OR Trump',\n",
    "           'data science OR coding OR programming']\n",
    "moons = ['No Phase', 'Full Moon','Last Quarter','New Moon','First Quarter']\n",
    "\n",
    "\n",
    "############################\n",
    "#### Application layout ####\n",
    "app.layout = html.Div(children=[    ### whole page\n",
    "    html.H1(                        ### page header\n",
    "        children='Lunar Cycles & Human Behavior',\n",
    "        style={'textAlign': 'center'}\n",
    "    ),\n",
    "\n",
    "    ### two column area\n",
    "    html.Div([    \n",
    "\n",
    "        ####################################\n",
    "        # The area with the dropdown menus #\n",
    "        html.Div([\n",
    "\n",
    "            # Adding a dropdown menu\n",
    "            html.Div([\n",
    "                html.P('Twitter Search Phrases:'),\n",
    "                dcc.Dropdown(\n",
    "                    id='query-dropdown',\n",
    "                    options=[{'label': i, 'value': i} for i in queries],\n",
    "                    value='(no keywords entered)'  # default initial value\n",
    "                ),\n",
    "                html.P(' d', style={'color':'#FFFFFF'}),\n",
    "                html.P('Moon Phrase:'),\n",
    "                dcc.Dropdown(\n",
    "                    id='moon-dropdown',\n",
    "                    options=[{'label': i, 'value': i} for i in moons],\n",
    "                    value='No Phase'   # default initial value\n",
    "                ),\n",
    "                html.P(' d', style={'color':'#FFFFFF'}),\n",
    "                dcc.Markdown(markdown_paragraph)\n",
    "            ]),\n",
    "\n",
    "\n",
    "            # setting the layout of the dropdown DIV area\n",
    "            ], style = {'width': '20%',\n",
    "                'height': '49%',\n",
    "                'display': 'inline-block'\n",
    "            }\n",
    "        ),\n",
    "\n",
    "        html.Div([], style={'width': '5%', 'display': 'inline-block'}),  ## just a spacer\n",
    "\n",
    "\n",
    "        #############################\n",
    "        # The area with the display #\n",
    "\n",
    "        ### TABS ###\n",
    "        html.Div([\n",
    "            dcc.Tabs(id='tabs', value='tab-1', children=[\n",
    "                dcc.Tab(id='tab1', label='Daily Sentiment', value='tab-1'),\n",
    "                dcc.Tab(id='tab2', label='Word Frequencies', value='tab-2'),\n",
    "                dcc.Tab(id='tab3', label='Facebook Prophet Seasonality', value='tab-3'),\n",
    "            ]),\n",
    "\n",
    "            ## displayed below tabs ##\n",
    "            html.Div(id='tabs-content', children=[\n",
    "                dcc.Graph(id='tw-graph'),\n",
    "\n",
    "                dcc.Slider(\n",
    "                    id='freq-slider',\n",
    "                    min=1,\n",
    "                    max=1000,\n",
    "                    step=1,\n",
    "                    value=50\n",
    "                ),\n",
    "                html.Div(id='slider-output-container')\n",
    "            ]),\n",
    "            ],\n",
    "            style={'width': '75%', 'display': 'inline-block'}  ###setting the graph/tabs area to right of options\n",
    "        )\n",
    "    ])\n",
    "])\n",
    "\n",
    "\n",
    "####################################################\n",
    "##### Callbacks section for linking everything #####\n",
    "####################################################\n",
    "\n",
    "\n",
    "@app.callback(\n",
    "    Output('slider-output-container', 'children'),\n",
    "    [Input('freq-slider', 'value'),\n",
    "     Input('query-dropdown', 'value')])\n",
    "def update_output(selected_value, selected_query):\n",
    "    return f'Displaying the {selected_value} most frequent words from the query: \"{selected_query}\"'\n",
    "\n",
    "\n",
    "@app.callback(\n",
    "    Output('tw-graph', 'figure'),\n",
    "    [Input('query-dropdown', 'value'),\n",
    "     Input('moon-dropdown', 'value'),\n",
    "     Input('tabs', 'value'),\n",
    "     Input('freq-slider', 'value')])\n",
    "def update_graph(selected_query, selected_moon, selected_tab, selected_n):\n",
    "    tweets_df = tw_df[tw_df['query'] == selected_query]\n",
    "    tweets_df.dropna(inplace=True)\n",
    "    if selected_tab == 'tab-1':\n",
    "        grouped = pd.DataFrame(tweets_df.groupby(['date', 'sentiment'])['tally'].sum()).reset_index()\n",
    "        traces = []\n",
    "        for sentiment in grouped.sentiment.unique():\n",
    "            temp_df = grouped[grouped.sentiment == sentiment]\n",
    "            traces.append(go.Scatter(\n",
    "                                x=temp_df.date,\n",
    "                                y=temp_df['tally'],\n",
    "                                name=sentiment,\n",
    "                                text=temp_df['sentiment'],\n",
    "                                mode='lines',\n",
    "                                opacity=0.8))\n",
    "        figure = {'data': traces,\n",
    "            'layout': go.Layout(colorway=[\"#5E0DAC\", '#FF4F00', '#375CB1', '#FF7400', '#FFF400', '#FF0056'],\n",
    "                                #height=600,\n",
    "                                title=f\"Daily Sentiment at Midnight (UTC) for : '{selected_query}'\",\n",
    "                                xaxis={\"title\":\"Date\",\n",
    "                                       'rangeslider': {'visible': True},\n",
    "                                       'type': 'date'},\n",
    "                                yaxis={\"title\":\"Sentiment Quantity (~1,000/day total)\"})}\n",
    "        return figure\n",
    "\n",
    "    elif selected_tab == 'tab-2':\n",
    "        df2 = tw_word_freqs_df[(tw_word_freqs_df['query'] == selected_query) &\n",
    "                               (tw_word_freqs_df['phase'] == selected_moon)][:selected_n]\n",
    "        ### plotting\n",
    "        trace = [go.Bar(x=df2['text'], y=df2['count'], name='', )]\n",
    "        figure = {'data': trace,\n",
    "            'layout': go.Layout(title=f\"Top {selected_n} Words for {selected_moon} at Midnight UTC\",\n",
    "                hovermode=\"closest\",\n",
    "                xaxis={\n",
    "                    'title': f\"Search Keywords: {selected_query}\", \n",
    "                    'titlefont': {'color': 'black', 'size': 14},\n",
    "                    'tickfont': {'size': 11, 'color': 'black'}},\n",
    "                yaxis={'tickfont': {'color': 'black'}}\n",
    "            )\n",
    "        }\n",
    "        return figure\n",
    "\n",
    "    elif selected_tab == 'tab-3':\n",
    "        #prepare data for FBProphet\n",
    "        tw_df = pd.read_csv('tweets_happysad_2018.csv')\n",
    "        tw_df.timestamp = pd.to_datetime(tw_df.timestamp, format='%Y%m%d')\n",
    "        grouped = pd.DataFrame(tw_df.groupby(['timestamp', 'sentiment'])['tally'].sum()).reset_index()\n",
    "        grp_pos = grouped[grouped.sentiment == 'positive'].drop('sentiment', axis=1).reset_index(drop=True)\n",
    "        grp_pos.columns = ['ds','y']\n",
    "\n",
    "        #make model and forecast\n",
    "        m = Prophet(holidays=phases)\n",
    "        m.fit(grp_pos)\n",
    "        future = m.make_future_dataframe(periods=60)\n",
    "        forecast = m.predict(future)\n",
    "\n",
    "        #plot holidays (lunar seasonalities)\n",
    "        figure = plot_holidays_component_plotly(m, forecast, figsize=(900, 300))   ### remove figsize?\n",
    "        return figure\n",
    "\n",
    "\n",
    "\n",
    "# automatically update HTML display if a change is made to code\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-capstone-env] *",
   "language": "python",
   "name": "conda-env-.conda-capstone-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
